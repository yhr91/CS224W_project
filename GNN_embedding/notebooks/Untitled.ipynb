{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name is 04-07_17-55-22\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/NCBI_to_UNIPROT.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e49aff6c099b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Experiment name is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expt_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e49aff6c099b>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(args, num_folds)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0medgelist_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medgelist_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/CS224W_project/GNN_embedding/load_assoc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, edgelist_file, use_features)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversions_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_ID_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgene_to_disease_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisease_to_genes_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/CS224W_project/GNN_embedding/load_assoc.py\u001b[0m in \u001b[0;36mread_ID_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;34m'''NOTE: used for uniprot features'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_ID_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../Data/NCBI_to_UNIPROT.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/NCBI_to_UNIPROT.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    @author: Ben, Yusuf, Kendrick\n",
    "    Trains models to predict disease gene associations.\n",
    "    \"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from load_assoc import ProcessData\n",
    "#from neural_net import GNN\n",
    "#import utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "#import conv_layers\n",
    "#import optimizers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainer(args, num_folds=5):\n",
    "    edgelist_file = args['dataset']\n",
    "\n",
    "    processed_data = ProcessData(edgelist_file, use_features=args['use_features'])\n",
    "    X = processed_data.X\n",
    "    X = torch.tensor(X.values, dtype=torch.float)\n",
    "    disease_test_scores = {}\n",
    "\n",
    "\n",
    "    # This returns all disease indices corresponding to given disease classes\n",
    "    sel_diseases = processed_data.get_disease_class_idx(['cancer'])\n",
    "    processed_data.Y = processed_data.Y.iloc[:,sel_diseases]\n",
    "\n",
    "    for ind, column in enumerate(processed_data.Y):\n",
    "        #if ind > 5: break # TODO: Remove this later on. For testing purposes only\n",
    "        \n",
    "        y = processed_data.Y[column].tolist()\n",
    "        edges = processed_data.get_edges()\n",
    "        \n",
    "        y = torch.tensor(np.array(y).astype('int'), dtype=torch.long)\n",
    "        edges = torch.tensor(edges.values, dtype=torch.long)\n",
    "        \n",
    "        # Set up train and test sets:\n",
    "        test_size = .1\n",
    "        #data_generator = load_pyg(X, edges, y,\n",
    "        #                                folds=num_folds, test_size=test_size)\n",
    "            \n",
    "        return X, edges, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dt = str(datetime.now())[5:19].replace(' ', '_').replace(':', '-')\n",
    "    \n",
    "    args={}\n",
    "    args['network-type']='GCNConv'\n",
    "    args['dataset']='../dataset_collection/GNBR-edgelist.csv'\n",
    "    args['expt_name']=dt\n",
    "    args['use_features']=True\n",
    "\n",
    "    print('Experiment name is', args['expt_name'])\n",
    "    x = trainer(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pyg(X, edges, y, folds=5, test_size=0.1):\n",
    "\n",
    "    # First identify the test set\n",
    "    indices = np.arange(len(X))\n",
    "    _,test_idx = train_test_split(indices, test_size=test_size,\n",
    "                                             stratify=y, random_state=40)\n",
    "\n",
    "    # Now use the remainder of the data to create train and val sets\n",
    "    kf = StratifiedKFold(n_splits=folds, random_state=2)\n",
    "\n",
    "    # Consider all edges and their reverse\n",
    "    edges = [e for e in edges.numpy() if e[0] != e[1]] # Remove self edges\n",
    "    reverse_edges = np.flip(np.array(edges),1)\n",
    "    edges = np.concatenate([edges,reverse_edges])\n",
    "    edges = np.unique(edges,axis=1) # Remove repeats\n",
    "    #edges = torch.tensor(edges, dtype=torch.long)\n",
    "                    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_pyg(x[0], x[1], x[2], folds=5, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  465, 10003],\n",
       "       [  465,   466],\n",
       "       [  465,  2024],\n",
       "       ...,\n",
       "       [19156, 19676],\n",
       "       [13731, 13730],\n",
       "       [13743, 13742]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389266"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([], size=(2, 0)),\n",
       "       values=tensor([], size=(0,)),\n",
       "       size=(2, 3), nnz=0, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse.FloatTensor(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
